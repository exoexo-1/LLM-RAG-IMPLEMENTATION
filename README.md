# ğŸ“˜ **AmbedkarGPT â€” Intern Task (RAG + Ollama + LangChain)**

A simple **command-line Q&A system** built as part of the **Kalpit Pvt Ltd â€“ AI Intern Hiring Assignment**.
The system uses a **Retrieval-Augmented Generation (RAG)** pipeline to answer questions **strictly based on Dr. B.R. Ambedkarâ€™s speech**.

This project:

* Loads *speech.txt*
* Splits it into chunks
* Embeds using **HuggingFace (all-MiniLM-L6-v2)**
* Stores vectors in **ChromaDB**
* Retrieves relevant chunks
* Sends context + question to **Ollama (Mistral 7B)**
* Answers only from the provided context
* Falls back to *â€œI donâ€™t knowâ€* if answer is not in the text

---

# ğŸ“‚ **Project Structure**

```
AmbedkarGPT-Intern-Task/
â”‚â”€â”€ db/                    # Auto-generated Chroma vector store
â”‚â”€â”€ venv/                  # Your virtual environment (ignored in GitHub)
â”‚â”€â”€ DrAmbedkar's Speach.ipynb   # Development notebook
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ speech.txt             # Provided Ambedkar speech
â”‚â”€â”€ README.md              # You're reading it :)
```

---

# âš™ï¸ **Installation & Setup**

## 1ï¸âƒ£ **Clone the Repository**

```bash
git clone https://github.com/exoexo-1/LLM-RAG-IMPLEMENTATION
cd AmbedkarGPT-Intern-Task
```

---

## 2ï¸âƒ£ **Create Virtual Environment**

```bash
python -m venv venv
source venv/bin/activate        # Mac/Linux
venv\Scripts\activate           # Windows
```

---

## 3ï¸âƒ£ **Install Dependencies**

```bash
pip install -r requirements.txt
```

This installs:

* langchain
* langchain-community
* sentence-transformers
* chromadb
* torch
* ollama
* requests

---

## 4ï¸âƒ£ **Install & Setup Ollama (Important)**

### **Download Ollama**

Mac/Linux:

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Windows:
Download from â†’ [https://ollama.com/download](https://ollama.com/download)

---

### **Pull the Mistral 7B Model**

```bash
ollama pull mistral
```

---

### **Start Ollama**

```bash
ollama serve
```

You must keep this running while using the script.

---

# ğŸ§  **How the System Works (RAG Pipeline)**

1. **speech.txt** is loaded
2. Text is split into chunks (300 chars, 100 overlap)
3. Embeddings created using `all-MiniLM-L6-v2`
4. Stored in **Chroma vector database**
5. User asks a question
6. System retrieves top 2 relevant chunks
7. Builds a prompt:

```
Use ONLY this context:
<retrieved chunks>

Question: <user question>
```

8. Sends prompt to **Ollama Mistral**
9. Prints the answer

---

# â–¶ï¸ **Running the Chat Application**

Simply run all cells of file  `DrAmbedkar's Speach.ipynb`.



---

# ğŸ’¬ **Example Usage**

```
Enter your question about Dr. B.R. Ambedkar's speeches:
> What is the real enemy according to the speech?

Assistant:
The speech states that "The real enemy is the belief in the shastras."
```

Out-of-context example:

```
> When was Ambedkar born?

Assistant:
I don't know.
```

---

# ğŸ§ª **Test Questions**

To verify your system:

* *What is the real enemy?*
* *Why canâ€™t someone believe in shastras and oppose caste?*
* *What does the speech say about social reform?*
* *What is compared to a gardenerâ€™s work?*
* *Who were Ambedkarâ€™s contemporaries?* â†’ Should answer: **I don't know**

---

# ğŸ **Submission Requirements (all satisfied)**

âœ” `speech.txt`
âœ” `requirements.txt`
âœ” Functional RAG pipeline
âœ” Uses LangChain + Chroma + HuggingFace embeddings
âœ” Uses Ollama Mistral 7B
âœ” Notebook/code well-commented
âœ” Command-line chat loop implemented
âœ” README explaining setup and usage


---

# ğŸ™Œ **Author**

Lakshya Agrawal
AI Intern Candidate â€” Kalpit Pvt Ltd

---

